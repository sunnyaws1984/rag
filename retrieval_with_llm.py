# retrieval_with_llm.py
# pip install gradio google-generativeai sentence-transformers langchain faiss-cpu python-dotenv

import os
import gradio as gr
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
import google.generativeai as genai

# 1️⃣ Load Gemini API key
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# 2️⃣ Load FAISS index with embeddings (must match embeddings used during creation)
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)

# 3️⃣ Create retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 4️⃣ Function to answer questions using retrieval + LLM
def answer_question(query):
    # Retrieve relevant documents
    results = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in results])
    
    # Generate answer using Gemini
    prompt = f"Use the context below to answer the question.\n\nContext:\n{context}\n\nQuestion: {query}"
    model = genai.GenerativeModel("gemini-2.0-flash")
    response = model.generate_content(prompt)
    
    return response.text

# 5️⃣ Gradio UI
iface = gr.Interface(
    fn=answer_question,
    inputs="text",
    outputs="text",
    title="RAG with Gemini Demo",
    description="Ask a question and get answers generated by Gemini using retrieved documents."
)
iface.launch()